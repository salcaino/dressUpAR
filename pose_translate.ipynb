{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OS2KhCZ7Uqt_",
        "outputId": "6b9bb555-9772-4b03-a71e-05bc10496195"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pillow==8.1.0 in /usr/local/lib/python3.7/dist-packages (8.1.0)\n",
            "Requirement already satisfied: matplotlib==3.3.4 in /usr/local/lib/python3.7/dist-packages (3.3.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4) (3.0.8)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4) (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4) (1.19.3)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4) (8.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib==3.3.4) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib==3.3.4) (1.15.0)\n",
            "Requirement already satisfied: numpy==1.19.3 in /usr/local/lib/python3.7/dist-packages (1.19.3)\n",
            "Requirement already satisfied: opencv-python==4.5.1.48 in /usr/local/lib/python3.7/dist-packages (4.5.1.48)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python==4.5.1.48) (1.19.3)\n",
            "Requirement already satisfied: tqdm==4.56.0 in /usr/local/lib/python3.7/dist-packages (4.56.0)\n",
            "/bin/bash: -c: line 1: syntax error: unexpected end of file\n",
            "Requirement already satisfied: mediapipe==0.8.3 in /usr/local/lib/python3.7/dist-packages (0.8.3)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from mediapipe==0.8.3) (1.0.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from mediapipe==0.8.3) (4.5.1.48)\n",
            "Requirement already satisfied: numpy==1.19.3 in /usr/local/lib/python3.7/dist-packages (from mediapipe==0.8.3) (1.19.3)\n",
            "Requirement already satisfied: protobuf>=3.11.4 in /usr/local/lib/python3.7/dist-packages (from mediapipe==0.8.3) (3.17.3)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from mediapipe==0.8.3) (0.37.1)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.7/dist-packages (from mediapipe==0.8.3) (0.6)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from mediapipe==0.8.3) (21.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from mediapipe==0.8.3) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pillow==8.1.0\n",
        "!pip install matplotlib==3.3.4\n",
        "!pip install numpy==1.19.3\n",
        "!pip install opencv-python==4.5.1.48\n",
        "!pip install tqdm==4.56.0\n",
        "!pip install requests==2.25.1|\n",
        "\n",
        "!pip install mediapipe==0.8.3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgMixROQCAmz",
        "outputId": "7ffe9b66-b7d2-4460-ac6d-cba5733e4429"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "T-Y464dejG2R"
      },
      "outputs": [],
      "source": [
        "!cp -r gdrive/MyDrive/VisionLab2Project/data ./\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2g-Lz31aWPi"
      },
      "source": [
        "## Mediapipe Pose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "V1JNMkWsk2B7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tqdm\n",
        "from mediapipe.python.solutions import pose as mp_pose\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.utils.data\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "import cv2 as cv\n",
        "from time import time\n",
        "from glob import glob\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "# Set our device:\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    torch.cuda.set_device(device)\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "class PoseTranslate(Dataset):\n",
        "    def __init__(self, basedir, imgset=\"train\", subset_size=30):\n",
        "        super(Dataset, self).__init__()\n",
        "\n",
        "        self.pose_tracker = mp_pose.Pose(upper_body_only=False)\n",
        "        self.basedir = basedir\n",
        "        path = os.path.join(self.basedir, \"subject_*\", \"body\", \"0000*\")\n",
        "        pose_list = glob(path)\n",
        "        pose_list = sorted(pose_list)\n",
        "        self.random_split = subset_size\n",
        "\n",
        "        size = len(pose_list)\n",
        "        trainset_size = int(size * 0.8)\n",
        "        if imgset == \"train\":\n",
        "            subject_list = pose_list[:trainset_size]\n",
        "        elif imgset == \"val\":\n",
        "            subject_list = pose_list[trainset_size:]\n",
        "\n",
        "        self.subjects = self.load_poses(subject_list)\n",
        "\n",
        "        print(f\"Dataset loaded {len(self.subjects)} subjects\")\n",
        "        \n",
        "    def read_batch_imgs(self, imgs_list):\n",
        "        batch = []\n",
        "        poses = torch.zeros((len(imgs_list), 33, 3))\n",
        "        for i, imgpath in enumerate(imgs_list):\n",
        "            loaded = cv.imread(imgpath, cv.IMREAD_COLOR)\n",
        "            result = self.pose_tracker.process(image=loaded)\n",
        "            pose_landmarks = result.pose_landmarks\n",
        "            if pose_landmarks is not None:\n",
        "                pose_landmarks = np.array([[lmk.x, lmk.y, lmk.z]\n",
        "                                        for lmk in pose_landmarks.landmark], dtype=np.float32)\n",
        "            else:\n",
        "                pose_landmarks = np.zeros((33,3))\n",
        "            pose_landmarks = torch.from_numpy(pose_landmarks)\n",
        "            poses[i] = pose_landmarks\n",
        "            batch.append(loaded)\n",
        "        return batch, poses\n",
        "\n",
        "    def load_poses(self, pose_list):\n",
        "      loaded = []\n",
        "      for pose_dir in pose_list:\n",
        "        img_list = sorted(glob(os.path.join(pose_dir, \"image\", \"*.jpg\")))\n",
        "        keypointspath = os.path.join(pose_dir, \"reconstruction\", \"smpl_parameter.txt\")\n",
        "        keypoints = np.loadtxt(keypointspath)\n",
        "        keypoints = keypoints[4:76]\n",
        "        keypoints = keypoints.reshape(24,3)\n",
        "        keypoints = torch.from_numpy(keypoints)\n",
        "        indices = np.arange(len(img_list))\n",
        "        randomsplit = np.sort(np.random.choice(indices, size=self.random_split, replace=False))\n",
        "        img_split = [img_list[j] for j in randomsplit]\n",
        "        batch, poses = self.read_batch_imgs(img_split)\n",
        "        all_keypoints = torch.zeros((poses.shape[0], keypoints.shape[0], keypoints.shape[1]))\n",
        "        all_keypoints[:] = keypoints\n",
        "\n",
        "        d = {\n",
        "          \"imgs\" : img_split,\n",
        "          \"batch\" : batch,\n",
        "          \"keypoints\" : all_keypoints,\n",
        "          \"poses\" : poses\n",
        "        }\n",
        "        loaded.append(d)\n",
        "      return loaded\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.subjects)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        d = self.subjects[index]\n",
        "        return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jwuk8XQtAdAL",
        "outputId": "451a01bf-1f50-4489-c34a-c6f55388f8ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded 48 subjects\n"
          ]
        }
      ],
      "source": [
        "dataset = PoseTranslate(basedir=\"data\", imgset=\"train\", subset_size=10)\n",
        "d = dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "AMl_Rk13AdAL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rvbt0-uLAdAL"
      },
      "outputs": [],
      "source": [
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_uniform_(m.weight.data)\n",
        "        nn.init.zeros_(m.bias.data)\n",
        "    if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
        "        nn.init.zeros_(m.bias.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "MLroKcWIAdAM"
      },
      "outputs": [],
      "source": [
        "class PoseTranslator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(PoseTranslator, self).__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(in_features=99, out_features=1024),\n",
        "      nn.BatchNorm1d(num_features=1024),\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout(0.3),\n",
        "      nn.Linear(in_features=1024, out_features=1024),\n",
        "      nn.BatchNorm1d(num_features=1024),\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout(0.3),\n",
        "      nn.Linear(in_features=1024, out_features=1024),\n",
        "      nn.BatchNorm1d(num_features=1024),\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout(0.3),\n",
        "      nn.Linear(in_features=1024, out_features=72),\n",
        "    )\n",
        "  \n",
        "  def forward(self, input):\n",
        "    out = self.layers(input)\n",
        "    out = out.reshape(-1, 24, 3)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayzq3VXZAdAM",
        "outputId": "5c26091e-1b32-4f4b-ca34-862fded78ca7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded 48 subjects\n",
            "Dataset loaded 12 subjects\n"
          ]
        }
      ],
      "source": [
        "batch_size = 1\n",
        "trainset = PoseTranslate(basedir=\"data\", imgset=\"train\", subset_size=30)\n",
        "trainloader = DataLoader(trainset, batch_size = batch_size, shuffle=True)\n",
        "valset = PoseTranslate(basedir=\"data\", imgset=\"val\", subset_size=30)\n",
        "valloader = DataLoader(valset, batch_size = batch_size, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "P9gqVLbRAdAN"
      },
      "outputs": [],
      "source": [
        "epoch_losses = []\n",
        "epoch_iou = []\n",
        "learning_rates = []\n",
        "val_losses = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "vWRA24Q-AdAN"
      },
      "outputs": [],
      "source": [
        "load = True\n",
        "path = \"translate_checkpoint.pt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sGQIealAdAN",
        "outputId": "040bf081-214f-4f0e-8390-5aa4dd327e11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "t_loss: 0.7416: : 3it [00:00, 26.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded with best val of 0.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "t_loss: 0.8297: : 48it [00:01, 25.98it/s]\n",
            "v_loss: 0.1828: : 12it [00:00, 27.02it/s]\n",
            "t_loss: 0.8717: : 48it [00:01, 27.06it/s]\n",
            "v_loss: 0.1994: : 12it [00:00, 27.45it/s]\n",
            "t_loss: 0.738: : 48it [00:02, 22.77it/s]\n",
            "v_loss: 0.1678: : 12it [00:00, 26.97it/s]\n",
            "t_loss: 0.7709: : 48it [00:01, 26.33it/s]\n",
            "v_loss: 0.2076: : 12it [00:00, 30.92it/s]\n",
            "t_loss: 0.6282: : 48it [00:01, 24.45it/s]\n",
            "v_loss: 0.2005: : 12it [00:00, 26.87it/s]\n",
            "t_loss: 0.7088: : 48it [00:02, 23.76it/s]\n",
            "v_loss: 0.2017: : 12it [00:00, 27.49it/s]\n",
            "t_loss: 0.7171: : 48it [00:01, 25.25it/s]\n",
            "v_loss: 0.1957: : 12it [00:00, 28.49it/s]\n",
            "t_loss: 0.797: : 48it [00:01, 24.16it/s]\n",
            "v_loss: 0.2391: : 12it [00:00, 22.28it/s]\n",
            "t_loss: 0.7576: : 48it [00:01, 24.35it/s]\n",
            "v_loss: 0.2062: : 12it [00:00, 25.72it/s]\n",
            "t_loss: 0.7059: : 48it [00:02, 21.45it/s]\n",
            "v_loss: 0.1928: : 12it [00:00, 22.54it/s]\n",
            "t_loss: 0.7481: : 48it [00:02, 20.57it/s]\n",
            "v_loss: 0.2009: : 12it [00:00, 24.12it/s]\n",
            "t_loss: 0.6959: : 48it [00:01, 25.39it/s]\n",
            "v_loss: 0.2006: : 12it [00:00, 24.82it/s]\n",
            "t_loss: 0.7032: : 48it [00:02, 22.93it/s]\n",
            "v_loss: 0.1869: : 12it [00:00, 27.57it/s]\n",
            "t_loss: 0.6927: : 48it [00:01, 26.87it/s]\n",
            "v_loss: 0.2122: : 12it [00:00, 30.36it/s]\n",
            "t_loss: 0.8014: : 48it [00:01, 27.49it/s]\n",
            "v_loss: 0.2498: : 12it [00:00, 29.80it/s]\n",
            "t_loss: 0.7241: : 48it [00:01, 26.87it/s]\n",
            "v_loss: 0.2312: : 12it [00:00, 30.59it/s]\n",
            "t_loss: 0.7553: : 48it [00:01, 24.98it/s]\n",
            "v_loss: 0.1865: : 12it [00:00, 28.81it/s]\n",
            "t_loss: 0.7143: : 48it [00:01, 27.12it/s]\n",
            "v_loss: 0.2074: : 12it [00:00, 28.85it/s]\n",
            "t_loss: 0.5944: : 48it [00:01, 27.79it/s]\n",
            "v_loss: 0.2359: : 12it [00:00, 28.98it/s]\n",
            "t_loss: 0.7092: : 48it [00:01, 28.78it/s]\n",
            "v_loss: 0.1858: : 12it [00:00, 26.90it/s]\n",
            "t_loss: 0.666: : 48it [00:02, 22.99it/s]\n",
            "v_loss: 0.2013: : 12it [00:00, 26.42it/s]\n",
            "t_loss: 0.6404: : 48it [00:01, 27.50it/s]\n",
            "v_loss: 0.2047: : 12it [00:00, 30.02it/s]\n",
            "t_loss: 0.6812: : 48it [00:01, 27.82it/s]\n",
            "v_loss: 0.2043: : 12it [00:00, 29.66it/s]\n",
            "t_loss: 0.7648: : 48it [00:01, 28.20it/s]\n",
            "v_loss: 0.248: : 12it [00:00, 26.92it/s]\n",
            "t_loss: 0.6747: : 48it [00:01, 25.98it/s]\n",
            "v_loss: 0.1898: : 12it [00:00, 27.91it/s]\n",
            "t_loss: 0.7963: : 48it [00:01, 26.63it/s]\n",
            "v_loss: 0.1934: : 12it [00:00, 29.36it/s]\n",
            "t_loss: 0.6679: : 48it [00:01, 26.45it/s]\n",
            "v_loss: 0.2043: : 12it [00:00, 31.11it/s]\n",
            "t_loss: 0.7698: : 48it [00:01, 27.90it/s]\n",
            "v_loss: 0.1668: : 12it [00:00, 29.76it/s]\n",
            "t_loss: 0.6728: : 48it [00:01, 26.77it/s]\n",
            "v_loss: 0.2037: : 12it [00:00, 26.66it/s]\n",
            "t_loss: 0.7023: : 48it [00:01, 25.88it/s]\n",
            "v_loss: 0.2041: : 12it [00:00, 24.66it/s]\n",
            "t_loss: 0.7118: : 48it [00:02, 23.52it/s]\n",
            "v_loss: 0.1807: : 12it [00:00, 28.31it/s]\n",
            "t_loss: 0.5821: : 48it [00:01, 25.13it/s]\n",
            "v_loss: 0.2101: : 12it [00:00, 28.93it/s]\n",
            "t_loss: 0.6845: : 48it [00:01, 25.69it/s]\n",
            "v_loss: 0.1794: : 12it [00:00, 25.80it/s]\n",
            "t_loss: 0.5695: : 48it [00:01, 24.93it/s]\n",
            "v_loss: 0.1949: : 12it [00:00, 29.02it/s]\n",
            "t_loss: 0.6648: : 48it [00:01, 25.37it/s]\n",
            "v_loss: 0.2039: : 12it [00:00, 28.70it/s]\n",
            "t_loss: 0.7273: : 48it [00:01, 25.98it/s]\n",
            "v_loss: 0.2456: : 12it [00:00, 26.94it/s]\n",
            "t_loss: 0.7203: : 48it [00:01, 26.79it/s]\n",
            "v_loss: 0.225: : 12it [00:00, 30.52it/s]\n",
            "t_loss: 0.6399: : 48it [00:01, 28.24it/s]\n",
            "v_loss: 0.1978: : 12it [00:00, 25.14it/s]\n",
            "t_loss: 0.7457: : 48it [00:02, 22.01it/s]\n",
            "v_loss: 0.1861: : 12it [00:00, 26.15it/s]\n",
            "t_loss: 0.739: : 48it [00:01, 29.13it/s]\n",
            "v_loss: 0.1877: : 12it [00:00, 29.86it/s]\n",
            "t_loss: 0.6665: : 48it [00:01, 25.61it/s]\n",
            "v_loss: 0.2487: : 12it [00:00, 27.50it/s]\n",
            "t_loss: 0.6346: : 48it [00:02, 22.08it/s]\n",
            "v_loss: 0.1906: : 12it [00:00, 27.18it/s]\n",
            "t_loss: 0.744: : 48it [00:01, 24.58it/s]\n",
            "v_loss: 0.1757: : 12it [00:00, 27.89it/s]\n",
            "t_loss: 0.6718: : 48it [00:01, 26.13it/s]\n",
            "v_loss: 0.1973: : 12it [00:00, 30.94it/s]\n",
            "t_loss: 0.7458: : 48it [00:01, 24.52it/s]\n",
            "v_loss: 0.2058: : 12it [00:00, 28.34it/s]\n",
            "t_loss: 0.7357: : 3it [00:00, 20.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving new best model with best val 0.19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "t_loss: 0.6434: : 48it [00:02, 22.33it/s]\n",
            "v_loss: 0.1859: : 12it [00:00, 26.99it/s]\n",
            "t_loss: 0.6798: : 48it [00:02, 21.83it/s]\n",
            "v_loss: 0.1867: : 12it [00:00, 25.57it/s]\n",
            "t_loss: 0.6572: : 48it [00:02, 17.35it/s]\n",
            "v_loss: 0.1931: : 12it [00:00, 24.14it/s]\n",
            "t_loss: 0.7816: : 48it [00:01, 26.01it/s]\n",
            "v_loss: 0.2499: : 12it [00:00, 27.74it/s]\n",
            "t_loss: 0.6344: : 48it [00:01, 27.44it/s]\n",
            "v_loss: 0.1915: : 12it [00:00, 31.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "finished with best val error of 0.19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "model = PoseTranslator().to(device)\n",
        "\n",
        "#learning rate\n",
        "lr = 25e-5\n",
        "# learning_rate = 25e-6\n",
        "weight_decay = 1e-4\n",
        "\n",
        "gamm = 0.7\n",
        "#number of training epochs\n",
        "epoch_n = 50\n",
        "\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "criterion = nn.MSELoss()\n",
        "# optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=weight_decay)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay) # Initialize the optimizer as SGD\n",
        "exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9, verbose=False)\n",
        "\n",
        "lrs = []\n",
        "\n",
        "best_val = -1\n",
        "if load:\n",
        "  \n",
        "  # model.load_state_dict(torch.load(path))\n",
        "  checkpoint = torch.load(path)\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  best_val = checkpoint['best_val']\n",
        "  print(f'Model loaded with best val of {best_val}')\n",
        "else:\n",
        "  model.apply(weights_init)\n",
        "\n",
        "\n",
        "for e in range(epoch_n):\n",
        "  epoch_loss = 0\n",
        "  model.train()\n",
        "  pbar = tqdm(enumerate(trainloader))\n",
        "  for i, data in pbar:\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    inputs = data['poses'].clone().squeeze_()\n",
        "    label = data['keypoints'].clone().squeeze_()\n",
        "    \n",
        "    inputs = inputs.to(device).requires_grad_(True)\n",
        "    label = label.to(device)\n",
        "\n",
        "    pred = model(inputs)\n",
        "    # pred = TF.resize(pred, (label.shape[-1]))\n",
        "    loss = criterion(pred, label)\n",
        "    loss.backward()\n",
        "    epoch_loss += loss.item()\n",
        "    optimizer.step()\n",
        "    # print('batch %d --- Loss: %.4f' % (i, loss.item() / batch_size))\n",
        "    batch_loss = round(loss.item() / batch_size, 4)\n",
        "    pbar.set_description(f\"t_loss: {batch_loss}\")\n",
        "  \n",
        "  epoch_loss = epoch_loss / len(trainset)\n",
        "  epoch_losses.append(epoch_loss)\n",
        "  learning_rates.append(exp_lr_scheduler.get_last_lr())\n",
        "  exp_lr_scheduler.step()\n",
        "\n",
        "  # print('Epoch %d / %d --- Loss: %.4f' % (e + 1, epoch_n, epoch_loss))\n",
        "\n",
        "  # torch.save(model.state_dict(), 'checkpoint.pt')\n",
        "  model.eval()\n",
        "\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  total_loss = 0\n",
        "  img_iou = []\n",
        "  # print('Running validation')\n",
        "  with torch.no_grad():\n",
        "    pbar = tqdm(enumerate(valloader))\n",
        "    for i, data in pbar:\n",
        "      inputs = data['poses'].clone()\n",
        "      inputs.squeeze_()\n",
        "      inputs = inputs.to(device).requires_grad_(True)\n",
        "      \n",
        "      label = data['keypoints'].clone()\n",
        "      label.squeeze_()\n",
        "      label = label.to(device)\n",
        "      \n",
        "      pred = model(inputs)\n",
        "      vloss = criterion(pred, label)  \n",
        "      batchloss = round(vloss.item(), 4)\n",
        "      total_loss += batchloss\n",
        "      pbar.set_description(f\"v_loss: {batchloss}\")\n",
        "    val_loss = total_loss / (len(valloader))\n",
        "    val_losses.append(val_loss)\n",
        "    \n",
        "  \n",
        "  # print(f\"Val loss: {round(val_loss, 4)}\")\n",
        "  val_loss = round(val_loss, 2)\n",
        "  if val_loss < best_val or best_val < 0:\n",
        "    best_val = val_loss\n",
        "    print(f\"Saving new best model with best val {best_val}\")\n",
        "    torch.save({\n",
        "            'epoch': e,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'best_val' : best_val\n",
        "            }, path)\n",
        "\n",
        "print(f\"finished with best val error of {best_val}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp translate_checkpoint.pt gdrive/MyDrive/VisionLab2Project/"
      ],
      "metadata": {
        "id": "qf2IIBwyml_X"
      },
      "execution_count": 41,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "pose_translate.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}